{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMpbrcITwLGV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZglgSEAZ69Rd"
      },
      "source": [
        "Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YorasnoQu8OS"
      },
      "outputs": [],
      "source": [
        "datasets =[\"bear\",\"camel\",\"cat\",\"cow\",\"crocodile\",\"dog\",\"elephant\",\"flamingo\",\n",
        "           \"giraffe\",\"hedgehog\",\"horse\",\"kangaroo\",\"lion\",\"monkey\",\n",
        "           \"owl\",\"panda\",\"penguin\",\"pig\",\"raccoon\",\"rhinoceros\",\n",
        "           \"sheep\",\"squirrel\",\"tiger\",\"whale\",\"zebra\"]\n",
        "train_X =[]\n",
        "train_y = []\n",
        "test_X = []\n",
        "test_y = []\n",
        "#valid_X = []\n",
        "#valid_y = []\n",
        "\n",
        "label = 0\n",
        "for dataset in datasets:\n",
        "  \n",
        "  data_filepath = \"/content/drive/MyDrive/dataset/sketchrnn_{dataset}.npz\".format(dataset=dataset)\n",
        "  file = np.load(data_filepath, encoding='latin1', allow_pickle=True)\n",
        "  \n",
        "  #test_X.extend(file[\"test\"])  \n",
        "  #train_X.extend(file[\"train\"])\n",
        "  #valid_X.extend(file[\"valid\"])\n",
        "\n",
        "  test_y.extend([label for i in range(file[\"test\"].shape[0])])\n",
        "  train_y.extend([label for i in range(file[\"train\"].shape[0])])\n",
        "  #valid_y.extend([label for i in range(file[\"valid\"].shape[0])])\n",
        "\n",
        "  label += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vGmCSVS_sq3"
      },
      "source": [
        "https://github.com/CMACH508/RPCL-pix2seq/blob/main/sample.py\n",
        "\n",
        "Tranform the data into 28x28 images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v4J0KdN6u5N",
        "outputId": "7a8276cd-0cd2-4c43-8b9e-b9a795929f17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting svgwrite\n",
            "  Downloading svgwrite-1.4.2-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 3.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: svgwrite\n",
            "Successfully installed svgwrite-1.4.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cairosvg\n",
            "  Downloading CairoSVG-2.5.2-py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from cairosvg) (7.1.2)\n",
            "Collecting cssselect2\n",
            "  Downloading cssselect2-0.6.0-py3-none-any.whl (15 kB)\n",
            "Collecting cairocffi\n",
            "  Downloading cairocffi-1.3.0.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tinycss2 in /usr/local/lib/python3.7/dist-packages (from cairosvg) (1.1.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from cairosvg) (0.7.1)\n",
            "Requirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from cairocffi->cairosvg) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1.0->cairocffi->cairosvg) (2.21)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from cssselect2->cairosvg) (0.5.1)\n",
            "Building wheels for collected packages: cairocffi\n",
            "  Building wheel for cairocffi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cairocffi: filename=cairocffi-1.3.0-py3-none-any.whl size=89668 sha256=e647dc884e0e5240e2619036f668be1ac850dcfbd43d5f98fb977e6b4135ce5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/e1/5c8a9692a27f639a07c949044bec943f26c81cd53d3805319f\n",
            "Successfully built cairocffi\n",
            "Installing collected packages: cssselect2, cairocffi, cairosvg\n",
            "Successfully installed cairocffi-1.3.0 cairosvg-2.5.2 cssselect2-0.6.0\n",
            "bear done\n",
            "camel done\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "! pip install svgwrite\n",
        "!pip install cairosvg\n",
        "import svgwrite\n",
        "import cairosvg\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "def get_bounds(data):\n",
        "  # Return bounds of data.\n",
        "  min_x = 0\n",
        "  max_x = 0\n",
        "  min_y = 0\n",
        "  max_y = 0\n",
        "\n",
        "  abs_x = 0\n",
        "  abs_y = 0\n",
        "  for i in range(len(data)):\n",
        "      x = float(data[i, 0])\n",
        "      y = float(data[i, 1])\n",
        "      abs_x += x\n",
        "      abs_y += y\n",
        "      min_x = min(min_x, abs_x)\n",
        "      min_y = min(min_y, abs_y)\n",
        "      max_x = max(max_x, abs_x)\n",
        "      max_y = max(max_y, abs_y)\n",
        "\n",
        "  return (min_x, max_x, min_y, max_y)\n",
        "\n",
        "\n",
        "\n",
        "def draw_strokes(data, svg_filename='sample.svg', width=48, margin=1.5, color='black'):\n",
        "  # convert sequence data to svg format\n",
        "  min_x, max_x, min_y, max_y = get_bounds(data)\n",
        "  if max_x - min_x > max_y - min_y:\n",
        "      norm = max_x - min_x\n",
        "      border_y = (norm - (max_y - min_y)) * 0.5\n",
        "      border_x = 0\n",
        "  else:\n",
        "      norm = max_y - min_y\n",
        "      border_x = (norm - (max_x - min_x)) * 0.5\n",
        "      border_y = 0\n",
        "\n",
        "  # normalize data\n",
        "  norm = max(norm, 10e-6)\n",
        "  scale = (width - 2*margin) / norm\n",
        "  dx = 0 - min_x + border_x\n",
        "  dy = 0 - min_y + border_y\n",
        "\n",
        "  abs_x = (0 + dx) * scale + margin\n",
        "  abs_y = (0 + dy) * scale + margin\n",
        "\n",
        "  # start converting\n",
        "  dwg = svgwrite.Drawing(svg_filename, size=(width,width))\n",
        "  dwg.add(dwg.rect(insert=(0, 0), size=(width,width),fill='white'))\n",
        "  lift_pen = 1\n",
        "  p = \"M%s,%s \" % (abs_x, abs_y)\n",
        "  command = \"m\"\n",
        "  for i in range(len(data)):\n",
        "      if (lift_pen == 1):\n",
        "          command = \"m\"\n",
        "      elif (command != \"l\"):\n",
        "          command = \"l\"\n",
        "      else:\n",
        "          command = \"\"\n",
        "      x = float(data[i,0]) * scale\n",
        "      y = float(data[i,1]) * scale\n",
        "      lift_pen = data[i, 2]\n",
        "      p += command+str(x)+\",\"+str(y)+\" \"\n",
        "  the_color = color  # \"black\"\n",
        "  stroke_width = 1\n",
        "  dwg.add(dwg.path(p).stroke(the_color,stroke_width).fill(\"none\"))\n",
        "  dwg.save()\n",
        "\n",
        "\n",
        "\n",
        "datasets =[\"bear\",\"camel\",\"cat\",\"cow\",\"crocodile\",\"dog\",\"elephant\",\"flamingo\",\n",
        "      \"giraffe\",\"hedgehog\",\"horse\",\"kangaroo\",\"lion\",\"monkey\",\n",
        "      \"owl\",\"panda\",\"penguin\",\"pig\",\"raccoon\",\"rhinoceros\",\n",
        "      \"sheep\",\"squirrel\",\"tiger\",\"whale\",\"zebra\"]\n",
        "pixel_file_location = \"/content/drive/MyDrive/dataset_pixel/\"\n",
        "\n",
        "for dataset in datasets:\n",
        "  data_filepath = \"/content/drive/MyDrive/dataset/sketchrnn_{dataset}.npz\".format(dataset=dataset)\n",
        "  file = np.load(data_filepath, encoding='latin1', allow_pickle=True)\n",
        "  # cut into train, test, valid\n",
        "  for tag in [\"train\",\"test\",\"valid\"]:\n",
        "    data = file[tag]\n",
        "    # create directory\n",
        "    pixel_file_path = pixel_file_location + \"/\" + tag + \"/\" + dataset\n",
        "    os.makedirs(pixel_file_path)\n",
        "\n",
        "    # iterate for each stroke\n",
        "    for i in range(len(data)):\n",
        "        svg_file_path = pixel_file_path + \"/\" + str(i) + \".svg\"\n",
        "        png_file_path = pixel_file_path + \"/\" + str(i) + \".png\"\n",
        "\n",
        "        # draw stroke\n",
        "        stroke_data = data[i]\n",
        "        draw_strokes(stroke_data,\n",
        "                      svg_filename=svg_file_path,\n",
        "                      width=28)\n",
        "\n",
        "        # convert svg to png\n",
        "        cairosvg.svg2png(url=svg_file_path, write_to=png_file_path)\n",
        "        \n",
        "        os.remove(svg_file_path)\n",
        "  print(dataset+' done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVJ1Ui2xBMjB"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torch.utils.data.dataloader as DataLoader\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzgU_VXvBCo1"
      },
      "outputs": [],
      "source": [
        "transform=transforms.Compose([transforms.Grayscale(),transforms.ToTensor()])\n",
        "\n",
        "train_X = []\n",
        "test_X = []\n",
        "train_y = []\n",
        "test_y = []\n",
        "\n",
        "\n",
        "# training set\n",
        "data_path_train =  \"/content/drive/MyDrive/dataset_pixel/\" + \"train\"\n",
        "\n",
        "i=0\n",
        "train_dataset = torchvision.datasets.ImageFolder(root=data_path_train,transform=transform)\n",
        "for image_data in train_dataset:\n",
        "  if i== 70000:#try less classes\n",
        "    break\n",
        "  train_X.append(image_data[0].detach().numpy()[0].reshape(28*28))\n",
        "  #train_y.append(image_data[1])\n",
        "  i+=1\n",
        "\n",
        "# test set\n",
        "data_path_test =  \"/content/drive/MyDrive//dataset_pixel/\" + \"test\"\n",
        "\n",
        "i=0\n",
        "test_dataset = torchvision.datasets.ImageFolder(root=data_path_test,transform=transform)\n",
        "for image_data in test_dataset:\n",
        "  if i== 25000: #try less classes\n",
        "    break\n",
        "  test_X.append(image_data[0].detach().numpy()[0].reshape(28*28))\n",
        "  #test_y.append(image_data[1])\n",
        "  i+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXtJj14LR4ws"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hPbzUN9oLm-"
      },
      "outputs": [],
      "source": [
        "train_X = np.array(train_X)\n",
        "test_X = np.array(test_X)\n",
        "\n",
        "test_y = []\n",
        "train_y = []\n",
        "for label in range(0,10):\n",
        "  train_y.extend([label for i in range(7000)])\n",
        "  test_y.extend([label for i in range(2500)])\n",
        "\n",
        "#train_y.sort()\n",
        "\n",
        "train_X, train_y = shuffle(train_X, train_y, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PZmBFs07ELG"
      },
      "source": [
        "MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkxRhfADDtTT"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "uQDbKsNJD4zQ",
        "outputId": "55b14e09-8f60-460d-a39f-b2f74868365f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hinge\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-7e39c95d492b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m70000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m25000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, classes, sample_weight)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0mcoef_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m             \u001b[0mintercept_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m         )\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, classes, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_call\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         )\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    332\u001b[0m         raise ValueError(\n\u001b[1;32m    333\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         )\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [5488000, 7000]"
          ]
        }
      ],
      "source": [
        "losses = [\"hinge\",\"log\"]#\n",
        "for l in losses:\n",
        "  print(l)\n",
        "  clf = SGDClassifier(loss=l,shuffle = False)\n",
        "  \n",
        "  for X,y in zip(np.array_split(train_X[:], 10),np.array_split(train_y[0:70000], 10)):\n",
        "    clf.partial_fit(X.reshape(-1,1),y,classes=[0,1,2,3,4,5,6,7,8,9])\n",
        " \n",
        "  y_pred = clf.predict(test_y[0:25000])\n",
        "  sc = clf.score(X=test_X[0:25000],y=test_y[0:25000])\n",
        "  f1 = f1_score(test_y, y_pred,average='macro')\n",
        "  print(f'score: {(sc*100):.2f}%')\n",
        "  print(f'f1-score: {(f1*100):.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc01tB83pxoM"
      },
      "outputs": [],
      "source": [
        "#crash when use full dataset\n",
        "kernels = [\"poly\",\"rbf\",\"sigmoid\"]\n",
        "for k in kernels:\n",
        "  print(k)\n",
        "  #for c in range(1,11):\n",
        "  svm = SVC(C=c, kernel=k)\n",
        "  svm.fit(train_X,train_y)\n",
        "  y_pred = svm.predict(test_y)\n",
        "  sc = svm.score(X=test_X,y=test_y)\n",
        "  f1 = f1_score(test_y, y_pred,average='macro')\n",
        "  print(f'score: {(sc*100):.2f}%')\n",
        "  print(f'f1-score: {(f1*100):.2f}%')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlXKxukTzjQJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "n = [256,200,128,100,64,50]\n",
        "for i in n:\n",
        "  print(n)\n",
        "  pca = PCA(n_components=2)\n",
        "  pca.fit(train_X)\n",
        "  X = pca.transform(train_X)\n",
        "  svm = SVC(C=1, kernel='rbf')\n",
        "  svm.fit(train_X,train_y)\n",
        "  y_pred = svm.predict(test_y)\n",
        "  sc = svm.score(X=test_X,y=test_y)\n",
        "  f1 = f1_score(test_y, y_pred,average='macro')\n",
        "  print(f'score: {(sc*100):.2f}%')\n",
        "  print(f'f1-score: {(f1*100):.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LY-faTdm4IF"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential()\n",
        "\n",
        "# model.add(tf.layers.conv1d(filters=48,\n",
        "#       kernel_size=5,activation=None,\n",
        "#       strides=1)\n",
        "\n",
        "# model.add(tf.layers.conv1d(filters=64,\n",
        "#       kernel_size=3,activation=None,\n",
        "#       strides=1))\n",
        "\n",
        "# model.add(tf.layers.conv1d(filters=96,\n",
        "#       kernel_size=3,activation=None,\n",
        "#       strides=1))\n",
        "\n",
        "model.add(tf.keras.layers.LSTM(16,return_sequences=True,input_shape=(None, 3)))\n",
        "model.add(tf.keras.layers.LSTM(16,return_sequences=True,input_shape=(None, 3)))\n",
        "\n",
        "# softmax classifier\n",
        "model.add(tf.keras.layers.Dense(.25))#25 classes\n",
        "model.add(tf.keras.layers.Activation(\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "2mltxbeG7XV8",
        "outputId": "bc5af1b4-fc95-45ec-e9bd-f240ad8db5c7"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a4d07e183519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m   \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Data cardinality is ambiguous:\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...\n  y sizes: 1750000\nMake sure all arrays contain the same number of samples."
          ]
        }
      ],
      "source": [
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "X_new = list(X_new)\n",
        "train_y = np.array(train_y)\n",
        "model.fit(X_new,train_y,batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npyP-n428p09"
      },
      "outputs": [],
      "source": [
        "y_pred = model.preidct(Xt_new)\n",
        "f1 = f1_score(test_y, y_pred,average='macro')\n",
        "print(f'f1-score: {(f1*100):.2f}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Machine Learning Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
